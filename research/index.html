<!DOCTYPE html>
<html lang="en">

  
  <head>    
    

    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Lei Li | Research</title>
    <meta name="author" content="Lei  Li" />
    <meta name="description" content="Lei Li" />


    
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:400,500,700|Roboto+Slab:400,500,700|Material+Icons">

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    
    
    <link rel="shortcut icon" href="/assets/img/favicon.ico"/>
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://craigleili.github.io/research/">
    
    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  
  <body class=" sticky-bottom-footer">

    
    <header>

      
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://craigleili.github.io/">Lei Li</a>
          
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              

              
              <li class="nav-item active">
                <a class="nav-link" href="/research/">Research<span class="sr-only">(current)</span></a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/misc/">Misc</a>
              </li>

              
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    
    <div class="container mt-5">
      
        <div class="post">
<article>
            
<h2 id="preprints">Preprints</h2>

<p>
  <a data-toggle="collapse" href="#CollapsePreprints" role="button" aria-expanded="false" aria-controls="CollapsePreprints">show...</a>
</p>
<div class="collapse" id="CollapsePreprints">
  <div class="publications">
  
  
    <ol class="bibliography"><li>
      <div class="row">
        <div class="col-sm-3 abbr">
<img src="/assets/img/2024-tsnts.png" class="teaser img-fluid z-dept-1"><abbr class="badge">arXiv</abbr>
</div>

        
        <div id="hadgi2024tsnts" class="col-sm-9">
        
          
          <div class="title"><strong>To Supervise or Not to Supervise: Understanding and Addressing the Key Challenges of 3D Transfer Learning</strong></div>
          
          <div class="author">Souhail Hadgi, 
                  <em>Lei Li</em>, and Maks Ovsjanikov
          </div>

          
          <div class="periodical">
            <em>arXiv</em> 2024
          </div>
        
          
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="https://arxiv.org/pdf/2403.17869.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          
          <div class="abstract hidden">
            <p>Transfer learning has long been a key factor in the advancement of many fields including 2D image analysis. Unfortunately, its applicability in 3D data processing has been relatively limited. While several approaches for 3D transfer learning have been proposed in recent literature, with contrastive learning gaining particular prominence, most existing methods in this domain have only been studied and evaluated in limited scenarios. Most importantly, there is currently a lack of principled understanding of both when and why 3D transfer learning methods are applicable. Remarkably, even the applicability of standard supervised pre-training is poorly understood. In this work, we conduct the first in-depth quantitative and qualitative investigation of supervised and contrastive pre-training strategies and their utility in downstream 3D tasks. We demonstrate that layer-wise analysis of learned features provides significant insight into the downstream utility of trained networks. Informed by this analysis, we propose a simple geometric regularization strategy, which improves the transferability of supervised pre-training. Our work thus sheds light onto both the specific challenges of 3D transfer learning, as well as strategies to overcome them.</p>
          </div>
        </div>
      </div>
</li></ol>
  
    <ol class="bibliography"></ol>
  
    <ol class="bibliography"></ol>
  
    <ol class="bibliography"></ol>
  
    <ol class="bibliography"></ol>
  
    <ol class="bibliography"></ol>
  
    <ol class="bibliography"></ol>
  
    <ol class="bibliography"></ol>
  
    <ol class="bibliography"></ol>
  
  </div>
</div>

<h2 id="publications">Publications</h2>

<div class="publications">


  
  <h2 class="year">2024</h2>
  <ol class="bibliography"><li>
      <div class="row">
        <div class="col-sm-3 abbr">
<img src="/assets/img/2023-genzi.jpg" class="teaser img-fluid z-dept-1"><abbr class="badge">CVPR</abbr>
</div>

        
        <div id="li2024genzi" class="col-sm-9">
        
          
          <div class="title"><strong>GenZI: Zero-Shot 3D Human-Scene Interaction Generation</strong></div>
          
          <div class="author">
                  <em>Lei Li</em>, and Angela Dai
          </div>

          
          <div class="periodical">
            <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> 2024
          </div>
        
          
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="https://arxiv.org/pdf/2311.17737.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://www.youtube.com/watch?v=ozfs6E0JIMY" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
            <a href="https://github.com/craigleili/GenZI" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="/projects/genzi/" class="btn btn-sm z-depth-0" role="button">Website</a>
          </div>

          
          <div class="abstract hidden">
            <p>Can we synthesize 3D humans interacting with scenes without learning from any 3D human-scene interaction data? We propose GenZI, the first zero-shot approach to generating 3D human-scene interactions. Key to GenZI is our distillation of interaction priors from large vision-language models (VLMs), which have learned a rich semantic space of 2D human-scene compositions. Given a natural language description and a coarse point location of the desired interaction in a 3D scene, we first leverage VLMs to imagine plausible 2D human interactions inpainted into multiple rendered views of the scene. We then formulate a robust iterative optimization to synthesize the pose and shape of a 3D human model in the scene, guided by consistency with the 2D interaction hypotheses. In contrast to existing learning-based approaches, GenZI circumvents the conventional need for captured 3D interaction data, and allows for flexible control of the 3D interaction synthesis with easy-to-use text prompts. Extensive experiments show that our zero-shot approach has high flexibility and generality, making it applicable to diverse scene types, including both indoor and outdoor environments.</p>
          </div>
        </div>
      </div>
</li></ol>

  
  <h2 class="year">2023</h2>
  <ol class="bibliography"><li>
      <div class="row">
        <div class="col-sm-3 abbr">
<img src="/assets/img/2023-vader.png" class="teaser img-fluid z-dept-1"><abbr class="badge">CVPR</abbr>
</div>

        
        <div id="attaiki2023localpc" class="col-sm-9">
        
          
          <div class="title"><strong>Generalizable Local Feature Pre-training for Deformable Shape Analysis</strong></div>
          
          <div class="author">Souhaib Attaiki, 
                  <em>Lei Li</em>, and Maks Ovsjanikov
          </div>

          
          <div class="periodical">
            <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> 2023
          </div>
        
          
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="https://arxiv.org/pdf/2303.15104.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/pvnieo/vader" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="#" class="btn btn-sm z-depth-0" role="button">Highlight Presentation</a>
          </div>

          
          <div class="abstract hidden">
            <p>Transfer learning is fundamental for addressing problems in settings with little training data. While several transfer learning approaches have been proposed in 3D, unfortunately, these solutions typically operate on an entire 3D object or even scene-level and thus, as we show, fail to generalize to new classes, such as deformable organic shapes. In addition, there is currently a lack of understanding of what makes pre-trained features transferable across significantly different 3D shape categories. In this paper, we make a step toward addressing these challenges. First, we analyze the link between feature locality and transferability in tasks involving deformable 3D objects, while also comparing different backbones and losses for local feature pre-training. We observe that with proper training, learned features can be useful in such tasks, but, crucially, only with an appropriate choice of the receptive field size. We then propose a differentiable method for optimizing the receptive field within 3D transfer learning. Jointly, this leads to the first learnable features that can successfully generalize to unseen classes of 3D shapes such as humans and animals. Our extensive experiments show that this approach leads to state-of-the-art results on several downstream tasks such as segmentation, shape correspondence, and classification. Our code is available at https://github.com/pvnieo/vader.</p>
          </div>
        </div>
      </div>
</li></ol>

  
  <h2 class="year">2022</h2>
  <ol class="bibliography">
<li>
      <div class="row">
        <div class="col-sm-3 abbr">
<img src="/assets/img/2022-attentivefmaps.png" class="teaser img-fluid z-dept-1"><abbr class="badge">NeurIPS</abbr>
</div>

        
        <div id="li2022attentivefmaps" class="col-sm-9">
        
          
          <div class="title"><strong>Learning Multi-resolution Functional Maps with Spectral Attention for Robust Shape Matching</strong></div>
          
          <div class="author">
                  <em>Lei Li</em>, Nicolas Donati, and Maks Ovsjanikov
          </div>

          
          <div class="periodical">
            <em>In Neural Information Processing Systems</em> 2022
          </div>
        
          
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="https://arxiv.org/pdf/2210.06373.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/craigleili/AttentiveFMaps" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="#" class="btn btn-sm z-depth-0" role="button">Spotlight Presentation</a>
          </div>

          
          <div class="abstract hidden">
            <p>In this work, we present a novel non-rigid shape matching framework based on multi-resolution functional maps with spectral attention. Existing functional map learning methods all rely on the critical choice of the spectral resolution hyperparameter, which can severely affect the overall accuracy or lead to overfitting, if not chosen carefully. In this paper, we show that spectral resolution tuning can be alleviated by introducing spectral attention. Our framework is applicable in both supervised and unsupervised settings, and we show that it is possible to train the network so that it can adapt the spectral resolution, depending on the given shape input. More specifically, we propose to compute multi-resolution functional maps that characterize correspondence across a range of spectral resolutions, and introduce a spectral attention network that helps to combine this representation into a single coherent final correspondence. Our approach is not only accurate with near-isometric input, for which a high spectral resolution is typically preferred, but also robust and able to produce reasonable matching even in the presence of significant non-isometric distortion, which poses great challenges to existing methods. We demonstrate the superior performance of our approach through experiments on a suite of challenging near-isometric and non-isometric shape matching benchmarks.</p>
          </div>
        </div>
      </div>
</li>
<li>
      <div class="row">
        <div class="col-sm-3 abbr">
<img src="/assets/img/2022-srfeat.png" class="teaser img-fluid z-dept-1"><abbr class="badge">3DV</abbr>
</div>

        
        <div id="li2022srfeat" class="col-sm-9">
        
          
          <div class="title"><strong>SRFeat: Learning Locally Accurate and Globally Consistent Non-Rigid Shape Correspondence</strong></div>
          
          <div class="author">
                  <em>Lei Li</em>, Souhaib Attaiki, and Maks Ovsjanikov
          </div>

          
          <div class="periodical">
            <em>In International Conference on 3D Vision</em> 2022
          </div>
        
          
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="http://www.lix.polytechnique.fr/%7Emaks/papers/SRFeat_3DV22.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://youtu.be/jXfjWcVjCeY" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
            <a href="https://github.com/craigleili/SRFeat" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          
          <div class="abstract hidden">
            <p>In this work, we present a novel learning-based framework that combines the local accuracy of contrastive learning with the global consistency of geometric approaches, for robust non-rigid matching. We first observe that while contrastive learning can lead to powerful point-wise features, the learned correspondences commonly lack smoothness and consistency, owing to the purely combinatorial nature of the standard contrastive losses. To overcome this limitation we propose to boost contrastive feature learning with two types of smoothness regularization that inject geometric information into correspondence learning. With this novel combination in hand, the resulting features are both highly discriminative across individual points, and, at the same time, lead to robust and consistent correspondences, through simple proximity queries. Our framework is general and is applicable to local feature learning in both the 3D and 2D domains. We demonstrate the superiority of our approach through extensive experiments on a wide range of challenging matching benchmarks, including 3D non-rigid shape correspondence and 2D image keypoint matching.</p>
          </div>
        </div>
      </div>
</li>
<li>
      <div class="row">
        <div class="col-sm-3 abbr">
<img src="/assets/img/2021-wsdesc.png" class="teaser img-fluid z-dept-1"><abbr class="badge">TVCG</abbr>
</div>

        
        <div id="li2022wsdesc" class="col-sm-9">
        
          
          <div class="title"><strong>WSDesc: Weakly Supervised 3D Local Descriptor Learning for Point Cloud Registration</strong></div>
          
          <div class="author">
                  <em>Lei Li</em>, Hongbo Fu, and Maks Ovsjanikov
          </div>

          
          <div class="periodical">
            <em>IEEE Transactions on Visualization and Computer Graphics</em> 2022
          </div>
        
          
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="https://arxiv.org/pdf/2108.02740.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/craigleili/WSDesc" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          
          <div class="abstract hidden">
            <p>In this work, we present a novel method called WSDesc to learn 3D local descriptors in a weakly supervised manner for robust point cloud registration. Our work builds upon recent 3D CNN-based descriptor extractors, which leverage a voxel-based representation to parameterize local geometry of 3D points. Instead of using a predefined fixed-size local support in voxelization, we propose to learn the optimal support in a data-driven manner. To this end, we design a novel differentiable voxelization layer that can back-propagate the gradient to the support size optimization. To train the extracted descriptors, we propose a novel registration loss based on the deviation from rigidity of 3D transformations, and the loss is weakly supervised by the prior knowledge that the input point clouds have partial overlap, without requiring ground-truth alignment information. Through extensive experiments, we show that our learned descriptors yield superior performance on existing geometric registration benchmarks.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  
  <h2 class="year">2021</h2>
  <ol class="bibliography"><li>
      <div class="row">
        <div class="col-sm-3 abbr">
<img src="/assets/img/2021-pointdsc.jpg" class="teaser img-fluid z-dept-1"><abbr class="badge">CVPR</abbr>
</div>

        
        <div id="bai2021pointdsc" class="col-sm-9">
        
          
          <div class="title"><strong>PointDSC: Robust Point Cloud Registration using Deep Spatial Consistency</strong></div>
          
          <div class="author">Xuyang Bai, Zixin Luo, Lei Zhou, Hongkai Chen, 
                  <em>Lei Li</em>, Zeyu Hu, Hongbo Fu, and Chiew-Lan Tai
          </div>

          
          <div class="periodical">
            <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> 2021
          </div>
        
          
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="https://arxiv.org/pdf/2103.05465.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/XuyangBai/PointDSC/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          
          <div class="abstract hidden">
            <p>Removing outlier correspondences is one of the critical steps for successful feature-based point cloud registration. Despite the increasing popularity of introducing deep learning methods in this field, spatial consistency, which is essentially established by a Euclidean transformation between point clouds, has received almost no individual attention in existing learning frameworks. In this paper, we present PointDSC, a novel deep neural network that explicitly incorporates spatial consistency for pruning outlier correspondences. First, we propose a nonlocal feature aggregation module, weighted by both feature and spatial coherence, for feature embedding of the input correspondences. Second, we formulate a differentiable spectral matching module, supervised by pairwise spatial compatibility, to estimate the inlier confidence of each correspondence from the embedded features. With modest computation cost, our method outperforms the state-of-the-art hand-crafted and learning-based outlier rejection approaches on several real-world datasets by a significant margin. We also show its wide applicability by combining PointDSC with different 3D local descriptors. </p>
          </div>
        </div>
      </div>
</li></ol>

  
  <h2 class="year">2020</h2>
  <ol class="bibliography">
<li>
      <div class="row">
        <div class="col-sm-3 abbr">
<img src="/assets/img/2020-lmvd.png" class="teaser img-fluid z-dept-1"><abbr class="badge">CVPR</abbr>
</div>

        
        <div id="li2020end" class="col-sm-9">
        
          
          <div class="title"><strong>End-to-End Learning Local Multi-view Descriptors for 3D Point Clouds</strong></div>
          
          <div class="author">
                  <em>Lei Li</em>, Siyu Zhu, Hongbo Fu, Ping Tan, and Chiew-Lan Tai
          </div>

          
          <div class="periodical">
            <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> 2020
          </div>
        
          
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_End-to-End_Learning_Local_Multi-View_Descriptors_for_3D_Point_Clouds_CVPR_2020_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/craigleili/3DLocalMultiViewDesc" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          
          <div class="abstract hidden">
            <p>In this work, we propose an end-to-end framework to learn local multi-view descriptors for 3D point clouds. To adopt a similar multi-view representation, existing studies use hand-crafted viewpoints for rendering in a preprocessing stage, which is detached from the subsequent descriptor learning stage. In our framework, we integrate the multi-view rendering into neural networks by using a differentiable renderer, which allows the viewpoints to be optimizable parameters for capturing more informative local context of interest points. To obtain discriminative descriptors, we also design a soft-view pooling module to attentively fuse convolutional features across views. Extensive experiments on existing 3D registration benchmarks show that our method outperforms existing local descriptors both quantitatively and qualitatively.</p>
          </div>
        </div>
      </div>
</li>
<li>
      <div class="row">
        <div class="col-sm-3 abbr">
<img src="/assets/img/2020-sketch-desc.png" class="teaser img-fluid z-dept-1"><abbr class="badge">TCSVT</abbr>
</div>

        
        <div id="yu2020sketchdesc" class="col-sm-9">
        
          
          <div class="title"><strong>SketchDesc: Learning Local Sketch Descriptors for Multi-view Correspondence</strong></div>
          
          <div class="author">Deng Yu, 
                  <em>Lei Li</em>, Youyi Zheng, Manfred Lau, Yi-Zhe Song, Chiew-Lan Tai, and Hongbo Fu
          </div>

          
          <div class="periodical">
            <em>IEEE Transactions on Circuits and Systems for Video Technology</em> 2020
          </div>
        
          
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="http://sweb.cityu.edu.hk/hongbofu/doc/SketchDesc_TCSVT2020.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          
          <div class="abstract hidden">
            <p>In this article, we study the problem of multi-view sketch correspondence, where we take as input multiple freehand sketches with different views of the same object and predict as output the semantic correspondence among the sketches. This problem is challenging since the visual features of corresponding points at different views can be very different. To this end, we take a deep learning approach and learn a novel local sketch descriptor from data. We contribute a training dataset by generating the pixel-level correspondence for the multi-view line drawings synthesized from 3D shapes. To handle the sparsity and ambiguity of sketches, we design a novel multi-branch neural network that integrates a patch-based representation and a multi-scale strategy to learn the pixel-level correspondence among multi-view sketches. We demonstrate the effectiveness of our proposed approach with extensive experiments on hand-drawn sketches and multi-view line drawings rendered from multiple 3D shape datasets.</p>
          </div>
        </div>
      </div>
</li>
<li>
      <div class="row">
        <div class="col-sm-3 abbr">
<img src="/assets/img/2020-sketch-r2cnn.png" class="teaser img-fluid z-dept-1"><abbr class="badge">TVCG</abbr>
</div>

        
        <div id="li2020sketch" class="col-sm-9">
        
          
          <div class="title"><strong>Sketch-R2CNN: An RNN-Rasterization-CNN Architecture for Vector Sketch Recognition</strong></div>
          
          <div class="author">
                  <em>Lei Li</em>, Changqing Zou, Youyi Zheng, Qingkun Su, Hongbo Fu, and Chiew-Lan Tai
          </div>

          
          <div class="periodical">
            <em>IEEE Transactions on Visualization and Computer Graphics</em> 2020
          </div>
        
          
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="http://sweb.cityu.edu.hk/hongbofu/doc/Sketch_R2CNN_preprint.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/craigleili/Sketch-R2CNN" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          
          <div class="abstract hidden">
            <p>Sketches in existing large-scale datasets like the recent QuickDraw collection are often stored in a vector format, with strokes consisting of sequentially sampled points. However, most existing sketch recognition methods rasterize vector sketches as binary images and then adopt image classification techniques. In this article, we propose a novel end-to-end single-branch network architecture RNN-Rasterization-CNN (Sketch-R2CNN for short) to fully leverage the vector format of sketches for recognition. Sketch-R2CNN takes a vector sketch as input and uses an RNN for extracting per-point features in the vector space. We then develop a neural line rasterization module to convert the vector sketch and the per-point features to multi-channel point feature maps, which are subsequently fed to a CNN for extracting convolutional features in the pixel space. Our neural line rasterization module is designed in a differentiable way for end-to-end learning. We perform experiments on existing large-scale sketch recognition datasets and show that the RNN-Rasterization design brings consistent improvement over CNN baselines and that Sketch-R2CNN substantially outperforms the state-of-the-art methods.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  
  
  
  <h2 class="year">2018</h2>
  <ol class="bibliography">
<li>
      <div class="row">
        <div class="col-sm-3 abbr">
<img src="/assets/img/2018-sketch-coclassification.png" class="teaser img-fluid z-dept-1"><abbr class="badge">Expressive</abbr>
</div>

        
        <div id="zhang2018context" class="col-sm-9">
        
          
          <div class="title"><strong>Context-based Sketch Classification</strong></div>
          
          <div class="author">Jianhui Zhang, Yilan Chen, 
                  <em>Lei Li</em>, Hongbo Fu, and Chiew-Lan Tai
          </div>

          
          <div class="periodical">
            <em>In Proceedings of the Joint Symposium on Computational Aesthetics and Sketch-Based Interfaces and Modeling and Non-Photorealistic Animation and Rendering</em> 2018
          </div>
        
          
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="http://sweb.cityu.edu.hk/hongbofu/doc/context_based_sketch_classification_Expressive2018.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://1drv.ms/v/s!Alg6Vpe53dEDgbdmcSOduFWkpB7_iA?e=wwa7zo" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
            <a href="https://github.com/EternalConfession/Context-Based-Sketch-Classification-Data" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          
          <div class="abstract hidden">
            <p>We present a novel context-based sketch classification framework using relations extracted from scene images. Most of existing methods perform sketch classification by considering individually sketched objects and often fail to identify their correct categories, due to the highly abstract nature of sketches. For a sketched scene containing multiple objects, we propose to classify a sketched object by considering its surrounding context in the scene, which provides vital cues for alleviating its recognition ambiguity. We learn such context knowledge from a database of scene images by summarizing the inter-object relations therein, such as co-occurrence, relative positions and sizes. We show that the context information can be used for both incremental sketch classification and sketch co-classification. Our method outperforms a state-of-the-art single-object classification method, evaluated on a new dataset of sketched scenes.</p>
          </div>
        </div>
      </div>
</li>
<li>
      <div class="row">
        <div class="col-sm-3 abbr">
<img src="/assets/img/2018-sketch-segmentation.png" class="teaser img-fluid z-dept-1"><abbr class="badge">CG&amp;A</abbr>
</div>

        
        <div id="li2018fast" class="col-sm-9">
        
          
          <div class="title"><strong>Fast Sketch Segmentation and Labeling with Deep Learning</strong></div>
          
          <div class="author">
                  <em>Lei Li</em>, Hongbo Fu, and Chiew-Lan Tai
          </div>

          
          <div class="periodical">
            <em>IEEE Computer Graphics and Applications</em> 2018
          </div>
        
          
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="http://sweb.cityu.edu.hk/hongbofu/doc/fast_sketch_segmentation_CGA2019.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://1drv.ms/v/s!Alg6Vpe53dEDgY0xQsIro4WtKx6G_w" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
            <a href="https://docs.google.com/forms/d/e/1FAIpQLSegEWbs2aE7E3arW8eSO8Bm2DNksvJ-P66RIw6YetJi2elpJQ/viewform?usp=sf_link" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="https://doi.ieeecomputersociety.org/10.1109/MCG.2020.3029599" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Best Paper Award Runner-Up</a>
          </div>

          
          <div class="abstract hidden">
            <p>We present a simple and efficient method based on deep learning to automatically decompose sketched objects into semantically valid parts. We train a deep neural network to transfer existing segmentations and labelings from three-dimensional (3-D) models to freehand sketches without requiring numerous well-annotated sketches as training data. The network takes the binary image of a sketched object as input and produces a corresponding segmentation map with per-pixel labelings as output. A subsequent postprocess procedure with multilabel graph cuts further refines the segmentation and labeling result. We validate our proposed method on two sketch datasets. Experiments show that our method outperforms the state-of-the-art method in terms of segmentation and labeling accuracy and is significantly faster, enabling further integration in interactive drawing systems. We demonstrate the efficiency of our method in a sketch-based modeling application that automatically transforms input sketches into 3-D models by part assembly.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  
  
  
  <h2 class="year">2016</h2>
  <ol class="bibliography"><li>
      <div class="row">
        <div class="col-sm-3 abbr"><img src="/assets/img/2016-sketch-reconstruct-teaser.png" class="teaser img-fluid z-dept-1"></div>

        
        <div id="li2016model" class="col-sm-9">
        
          
          <div class="title"><strong>Model-driven Sketch Reconstruction with Structure-oriented Retrieval</strong></div>
          
          <div class="author">
                  <em>Lei Li</em>, Zhe Huang, Changqing Zou, Chiew-Lan Tai, Rynson WH Lau, Hao Zhang, Ping Tan, and Hongbo Fu
          </div>

          
          <div class="periodical">
            <em>In SIGGRAPH ASIA Technical Briefs</em> 2016
          </div>
        
          
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="http://sweb.cityu.edu.hk/hongbofu/doc/sketch_reconstruction_brief_SA16.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://1drv.ms/b/s!Alg6Vpe53dEDlUmR9hkHmHB-I0Wc" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supplement</a>
          </div>

          
          <div class="abstract hidden">
            <p>We propose an interactive system that aims at lifting a 2D sketch into a 3D sketch with the help of existing models in shape collections. The key idea is to exploit part structure for shape retrieval and sketch reconstruction. We adopt sketch-based shape retrieval and develop a novel matching algorithm which considers structure in addition to traditional shape features. From a list of retrieved models, users select one to serve as a 3D proxy, providing abstract 3D information. Then our reconstruction method transforms the sketch into 3D geometry by back-projection, followed by an optimization procedure based on the Laplacian mesh deformation framework. Preliminary evaluations show that our retrieval algorithm is more effective than a state-of-the-art method and users can create interesting 3D forms of sketches without precise drawing skills.</p>
          </div>
        </div>
      </div>
</li></ol>

</div>

          </article>

        </div>

    </div>

        <footer class="sticky-bottom mt-5">
      <div class="container">
        © Copyright 2024 Lei  Li. Last updated: March 2024.
      </div>
    </footer>


    
    
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script>
  <script src="/assets/js/common.js"></script>

    
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-P3RT2Z6SLL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-P3RT2Z6SLL');
  </script>
  </body>
</html>

