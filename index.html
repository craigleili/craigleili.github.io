<!DOCTYPE html>
<html lang="en">

  
  <head>    
    

    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Lei Li</title>
    <meta name="author" content="Lei  Li" />
    <meta name="description" content="Lei Li" />


    
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:400,500,700|Roboto+Slab:400,500,700|Material+Icons">

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    
    
    <link rel="shortcut icon" href="/assets/img/favicon.ico"/>
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://craigleili.github.io/">
    
    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  
  <body class=" sticky-bottom-footer">

    
    <header>

      
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top">
        <div class="container">
          
          
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              
              <li class="nav-item active">
                <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a>
              </li>
              

              
              <li class="nav-item ">
                <a class="nav-link" href="/research/">Research</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/misc/">Misc</a>
              </li>

              
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    
    <div class="container mt-5">
      
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           Lei Li
          </h1>
          <p class="desc"><a href="https://www.3dunderstanding.org" target="_blank" rel="noopener noreferrer">3D AI Lab</a>, Technical University of Munich, Germany</p>
        </header>

        <article>
          <div class="profile float-right">
<figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/profile-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/profile-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/profile-1400.webp"></source>
    
    <img class="img-fluid rounded z-depth-1" src="/assets/img/profile.jpg" alt="profile.jpg">

  </picture><figcaption class="caption">On the Faculty Job Market!</figcaption>

</figure>

          </div>

          <div class="clearfix">
            <p>Lei Li is a postdoctoral researcher working with Prof. <a href="https://www.3dunderstanding.org/team.html" target="_blank" rel="noopener noreferrer">Angela Dai</a> at the 3D AI Lab, Technical University of Munich, Germany. Previously, he worked as a postdoctoral researcher with Prof. <a href="http://www.lix.polytechnique.fr/~maks/" target="_blank" rel="noopener noreferrer">Maks Ovsjanikov</a> from 2020 to 2022 at LIX, École Polytechnique / Inria, France. Lei earned his Ph.D. degree in computer science and engineering (2020) from The Hong Kong University of Science and Technology, and his B.Eng. degree in software engineering (2014) from Shandong University, China. He was a research intern at Alibaba A.I. Labs (2018) and Megvii Research (2019). His Ph.D. thesis advisor is Prof. <a href="https://cse.hkust.edu.hk/~taicl" target="_blank" rel="noopener noreferrer">Chiew-Lan Tai</a>, and he also works closely with Prof. <a href="http://sweb.cityu.edu.hk/hongbofu/index.htm" target="_blank" rel="noopener noreferrer">Hongbo Fu</a>. Lei’s research interests are Computer Graphics and Computer Vision, with a focus on geometric deep learning for shape analysis.</p>

          </div>

          
          <div class="social">
            <div class="contact-icons">
            <a href="mailto:%63%72%61%69%67%6C%65%69.%6C%69@%74%75%6D.%64%65" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://scholar.google.com/citations?user=uzh8LlIAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/craigleili?tab=repositories" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
            <a href="https://twitter.com/craigleili" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a>
            

            </div>

            <div class="contact-note">
              
            </div>
            
          </div>
                    
          <div class="news">
            <h2>News</h2>
            <div class="table-responsive">
              <table class="table table-sm table-borderless">
                
                <tr>
                  <th scope="row">02.2024</th>
                  <td>Our work <a href="https://craigleili.github.io/projects/genzi/"><strong>GenZI: Zero-Shot 3D Human-Scene Interaction Generation</strong></a> has been accepted by <em>CVPR 2024</em>.</td>
                </tr>
                <tr>
                  <th scope="row">01.2024</th>
                  <td>I was invited to serve as a member of the technical papers committee for the <em>2024 Eurographics Symposium on Geometry Processing</em>.</td>
                </tr>
                <tr>
                  <th scope="row">07.2023</th>
                  <td>I gave an invited talk, titled <em>Towards Robust Shape Correspondence: Learning with Receptive Field Optimization</em>, in the Group of Geometric Computation and Visualisation led by Prof. Shengjun Liu at Central South University, China.</td>
                </tr>
                <tr>
                  <th scope="row">03.2023</th>
                  <td>Postdoc at 3D AI Lab, Technical University of Munich.</td>
                </tr>
                <tr>
                  <th scope="row">02.2023</th>
                  <td>Our work <strong>Generalizable Local Feature Pre-training for Deformable Shape Analysis</strong> has been accepted by <em>CVPR 2023</em> and selected as a <em>highlight</em> (10% of accepted papers, 2.5% of submissions).</td>
                </tr>
                <tr>
                  <th scope="row"></th>
                  <td>
                    <a class="news-title" href="/news/">more...</a>
                  </td>
                </tr>
              </table>
            </div>
          </div>

                    <div class="publications">
            <h2>Selected Publications</h2>
            <ol class="bibliography">
<li>
      <div class="row">
        <div class="col-sm-3 abbr">
<img src="/assets/img/2023-genzi.jpg" class="teaser img-fluid z-dept-1"><abbr class="badge">CVPR</abbr>
</div>

        
        <div id="li2024genzi" class="col-sm-9">
        
          
          <div class="title"><strong>GenZI: Zero-Shot 3D Human-Scene Interaction Generation</strong></div>
          
          <div class="author">
                  <em>Lei Li</em>, and Angela Dai
          </div>

          
          <div class="periodical">
            <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> 2024
          </div>
        
          
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="https://arxiv.org/pdf/2311.17737.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://www.youtube.com/watch?v=ozfs6E0JIMY" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
            <a href="https://github.com/craigleili/GenZI" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="/projects/genzi/" class="btn btn-sm z-depth-0" role="button">Website</a>
          </div>

          
          <div class="abstract hidden">
            <p>Can we synthesize 3D humans interacting with scenes without learning from any 3D human-scene interaction data? We propose GenZI, the first zero-shot approach to generating 3D human-scene interactions. Key to GenZI is our distillation of interaction priors from large vision-language models (VLMs), which have learned a rich semantic space of 2D human-scene compositions. Given a natural language description and a coarse point location of the desired interaction in a 3D scene, we first leverage VLMs to imagine plausible 2D human interactions inpainted into multiple rendered views of the scene. We then formulate a robust iterative optimization to synthesize the pose and shape of a 3D human model in the scene, guided by consistency with the 2D interaction hypotheses. In contrast to existing learning-based approaches, GenZI circumvents the conventional need for captured 3D interaction data, and allows for flexible control of the 3D interaction synthesis with easy-to-use text prompts. Extensive experiments show that our zero-shot approach has high flexibility and generality, making it applicable to diverse scene types, including both indoor and outdoor environments.</p>
          </div>
        </div>
      </div>
</li>
<li>
      <div class="row">
        <div class="col-sm-3 abbr">
<img src="/assets/img/2023-vader.png" class="teaser img-fluid z-dept-1"><abbr class="badge">CVPR</abbr>
</div>

        
        <div id="attaiki2023localpc" class="col-sm-9">
        
          
          <div class="title"><strong>Generalizable Local Feature Pre-training for Deformable Shape Analysis</strong></div>
          
          <div class="author">Souhaib Attaiki, 
                  <em>Lei Li</em>, and Maks Ovsjanikov
          </div>

          
          <div class="periodical">
            <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> 2023
          </div>
        
          
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="https://arxiv.org/pdf/2303.15104.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/pvnieo/vader" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="#" class="btn btn-sm z-depth-0" role="button">Highlight Presentation</a>
          </div>

          
          <div class="abstract hidden">
            <p>Transfer learning is fundamental for addressing problems in settings with little training data. While several transfer learning approaches have been proposed in 3D, unfortunately, these solutions typically operate on an entire 3D object or even scene-level and thus, as we show, fail to generalize to new classes, such as deformable organic shapes. In addition, there is currently a lack of understanding of what makes pre-trained features transferable across significantly different 3D shape categories. In this paper, we make a step toward addressing these challenges. First, we analyze the link between feature locality and transferability in tasks involving deformable 3D objects, while also comparing different backbones and losses for local feature pre-training. We observe that with proper training, learned features can be useful in such tasks, but, crucially, only with an appropriate choice of the receptive field size. We then propose a differentiable method for optimizing the receptive field within 3D transfer learning. Jointly, this leads to the first learnable features that can successfully generalize to unseen classes of 3D shapes such as humans and animals. Our extensive experiments show that this approach leads to state-of-the-art results on several downstream tasks such as segmentation, shape correspondence, and classification. Our code is available at https://github.com/pvnieo/vader.</p>
          </div>
        </div>
      </div>
</li>
<li>
      <div class="row">
        <div class="col-sm-3 abbr">
<img src="/assets/img/2022-attentivefmaps.png" class="teaser img-fluid z-dept-1"><abbr class="badge">NeurIPS</abbr>
</div>

        
        <div id="li2022attentivefmaps" class="col-sm-9">
        
          
          <div class="title"><strong>Learning Multi-resolution Functional Maps with Spectral Attention for Robust Shape Matching</strong></div>
          
          <div class="author">
                  <em>Lei Li</em>, Nicolas Donati, and Maks Ovsjanikov
          </div>

          
          <div class="periodical">
            <em>In Neural Information Processing Systems</em> 2022
          </div>
        
          
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="https://arxiv.org/pdf/2210.06373.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/craigleili/AttentiveFMaps" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="#" class="btn btn-sm z-depth-0" role="button">Spotlight Presentation</a>
          </div>

          
          <div class="abstract hidden">
            <p>In this work, we present a novel non-rigid shape matching framework based on multi-resolution functional maps with spectral attention. Existing functional map learning methods all rely on the critical choice of the spectral resolution hyperparameter, which can severely affect the overall accuracy or lead to overfitting, if not chosen carefully. In this paper, we show that spectral resolution tuning can be alleviated by introducing spectral attention. Our framework is applicable in both supervised and unsupervised settings, and we show that it is possible to train the network so that it can adapt the spectral resolution, depending on the given shape input. More specifically, we propose to compute multi-resolution functional maps that characterize correspondence across a range of spectral resolutions, and introduce a spectral attention network that helps to combine this representation into a single coherent final correspondence. Our approach is not only accurate with near-isometric input, for which a high spectral resolution is typically preferred, but also robust and able to produce reasonable matching even in the presence of significant non-isometric distortion, which poses great challenges to existing methods. We demonstrate the superior performance of our approach through experiments on a suite of challenging near-isometric and non-isometric shape matching benchmarks.</p>
          </div>
        </div>
      </div>
</li>
<li>
      <div class="row">
        <div class="col-sm-3 abbr">
<img src="/assets/img/2021-wsdesc.png" class="teaser img-fluid z-dept-1"><abbr class="badge">TVCG</abbr>
</div>

        
        <div id="li2022wsdesc" class="col-sm-9">
        
          
          <div class="title"><strong>WSDesc: Weakly Supervised 3D Local Descriptor Learning for Point Cloud Registration</strong></div>
          
          <div class="author">
                  <em>Lei Li</em>, Hongbo Fu, and Maks Ovsjanikov
          </div>

          
          <div class="periodical">
            <em>IEEE Transactions on Visualization and Computer Graphics</em> 2022
          </div>
        
          
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="https://arxiv.org/pdf/2108.02740.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/craigleili/WSDesc" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          
          <div class="abstract hidden">
            <p>In this work, we present a novel method called WSDesc to learn 3D local descriptors in a weakly supervised manner for robust point cloud registration. Our work builds upon recent 3D CNN-based descriptor extractors, which leverage a voxel-based representation to parameterize local geometry of 3D points. Instead of using a predefined fixed-size local support in voxelization, we propose to learn the optimal support in a data-driven manner. To this end, we design a novel differentiable voxelization layer that can back-propagate the gradient to the support size optimization. To train the extracted descriptors, we propose a novel registration loss based on the deviation from rigidity of 3D transformations, and the loss is weakly supervised by the prior knowledge that the input point clouds have partial overlap, without requiring ground-truth alignment information. Through extensive experiments, we show that our learned descriptors yield superior performance on existing geometric registration benchmarks.</p>
          </div>
        </div>
      </div>
</li>
<li>
      <div class="row">
        <div class="col-sm-3 abbr">
<img src="/assets/img/2021-pointdsc.jpg" class="teaser img-fluid z-dept-1"><abbr class="badge">CVPR</abbr>
</div>

        
        <div id="bai2021pointdsc" class="col-sm-9">
        
          
          <div class="title"><strong>PointDSC: Robust Point Cloud Registration using Deep Spatial Consistency</strong></div>
          
          <div class="author">Xuyang Bai, Zixin Luo, Lei Zhou, Hongkai Chen, 
                  <em>Lei Li</em>, Zeyu Hu, Hongbo Fu, and Chiew-Lan Tai
          </div>

          
          <div class="periodical">
            <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> 2021
          </div>
        
          
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="https://arxiv.org/pdf/2103.05465.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/XuyangBai/PointDSC/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          
          <div class="abstract hidden">
            <p>Removing outlier correspondences is one of the critical steps for successful feature-based point cloud registration. Despite the increasing popularity of introducing deep learning methods in this field, spatial consistency, which is essentially established by a Euclidean transformation between point clouds, has received almost no individual attention in existing learning frameworks. In this paper, we present PointDSC, a novel deep neural network that explicitly incorporates spatial consistency for pruning outlier correspondences. First, we propose a nonlocal feature aggregation module, weighted by both feature and spatial coherence, for feature embedding of the input correspondences. Second, we formulate a differentiable spectral matching module, supervised by pairwise spatial compatibility, to estimate the inlier confidence of each correspondence from the embedded features. With modest computation cost, our method outperforms the state-of-the-art hand-crafted and learning-based outlier rejection approaches on several real-world datasets by a significant margin. We also show its wide applicability by combining PointDSC with different 3D local descriptors. </p>
          </div>
        </div>
      </div>
</li>
<li>
      <div class="row">
        <div class="col-sm-3 abbr">
<img src="/assets/img/2020-lmvd.png" class="teaser img-fluid z-dept-1"><abbr class="badge">CVPR</abbr>
</div>

        
        <div id="li2020end" class="col-sm-9">
        
          
          <div class="title"><strong>End-to-End Learning Local Multi-view Descriptors for 3D Point Clouds</strong></div>
          
          <div class="author">
                  <em>Lei Li</em>, Siyu Zhu, Hongbo Fu, Ping Tan, and Chiew-Lan Tai
          </div>

          
          <div class="periodical">
            <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> 2020
          </div>
        
          
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_End-to-End_Learning_Local_Multi-View_Descriptors_for_3D_Point_Clouds_CVPR_2020_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/craigleili/3DLocalMultiViewDesc" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          
          <div class="abstract hidden">
            <p>In this work, we propose an end-to-end framework to learn local multi-view descriptors for 3D point clouds. To adopt a similar multi-view representation, existing studies use hand-crafted viewpoints for rendering in a preprocessing stage, which is detached from the subsequent descriptor learning stage. In our framework, we integrate the multi-view rendering into neural networks by using a differentiable renderer, which allows the viewpoints to be optimizable parameters for capturing more informative local context of interest points. To obtain discriminative descriptors, we also design a soft-view pooling module to attentively fuse convolutional features across views. Extensive experiments on existing 3D registration benchmarks show that our method outperforms existing local descriptors both quantitatively and qualitatively.</p>
          </div>
        </div>
      </div>
</li>
<li>
      <div class="row">
        <div class="col-sm-3 abbr">
<img src="/assets/img/2020-sketch-r2cnn.png" class="teaser img-fluid z-dept-1"><abbr class="badge">TVCG</abbr>
</div>

        
        <div id="li2020sketch" class="col-sm-9">
        
          
          <div class="title"><strong>Sketch-R2CNN: An RNN-Rasterization-CNN Architecture for Vector Sketch Recognition</strong></div>
          
          <div class="author">
                  <em>Lei Li</em>, Changqing Zou, Youyi Zheng, Qingkun Su, Hongbo Fu, and Chiew-Lan Tai
          </div>

          
          <div class="periodical">
            <em>IEEE Transactions on Visualization and Computer Graphics</em> 2020
          </div>
        
          
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="http://sweb.cityu.edu.hk/hongbofu/doc/Sketch_R2CNN_preprint.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/craigleili/Sketch-R2CNN" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          
          <div class="abstract hidden">
            <p>Sketches in existing large-scale datasets like the recent QuickDraw collection are often stored in a vector format, with strokes consisting of sequentially sampled points. However, most existing sketch recognition methods rasterize vector sketches as binary images and then adopt image classification techniques. In this article, we propose a novel end-to-end single-branch network architecture RNN-Rasterization-CNN (Sketch-R2CNN for short) to fully leverage the vector format of sketches for recognition. Sketch-R2CNN takes a vector sketch as input and uses an RNN for extracting per-point features in the vector space. We then develop a neural line rasterization module to convert the vector sketch and the per-point features to multi-channel point feature maps, which are subsequently fed to a CNN for extracting convolutional features in the pixel space. Our neural line rasterization module is designed in a differentiable way for end-to-end learning. We perform experiments on existing large-scale sketch recognition datasets and show that the RNN-Rasterization design brings consistent improvement over CNN baselines and that Sketch-R2CNN substantially outperforms the state-of-the-art methods.</p>
          </div>
        </div>
      </div>
</li>
<li>
      <div class="row">
        <div class="col-sm-3 abbr">
<img src="/assets/img/2018-sketch-segmentation.png" class="teaser img-fluid z-dept-1"><abbr class="badge">CG&amp;A</abbr>
</div>

        
        <div id="li2018fast" class="col-sm-9">
        
          
          <div class="title"><strong>Fast Sketch Segmentation and Labeling with Deep Learning</strong></div>
          
          <div class="author">
                  <em>Lei Li</em>, Hongbo Fu, and Chiew-Lan Tai
          </div>

          
          <div class="periodical">
            <em>IEEE Computer Graphics and Applications</em> 2018
          </div>
        
          
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="http://sweb.cityu.edu.hk/hongbofu/doc/fast_sketch_segmentation_CGA2019.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://1drv.ms/v/s!Alg6Vpe53dEDgY0xQsIro4WtKx6G_w" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
            <a href="https://docs.google.com/forms/d/e/1FAIpQLSegEWbs2aE7E3arW8eSO8Bm2DNksvJ-P66RIw6YetJi2elpJQ/viewform?usp=sf_link" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="https://doi.ieeecomputersociety.org/10.1109/MCG.2020.3029599" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Best Paper Award Runner-Up</a>
          </div>

          
          <div class="abstract hidden">
            <p>We present a simple and efficient method based on deep learning to automatically decompose sketched objects into semantically valid parts. We train a deep neural network to transfer existing segmentations and labelings from three-dimensional (3-D) models to freehand sketches without requiring numerous well-annotated sketches as training data. The network takes the binary image of a sketched object as input and produces a corresponding segmentation map with per-pixel labelings as output. A subsequent postprocess procedure with multilabel graph cuts further refines the segmentation and labeling result. We validate our proposed method on two sketch datasets. Experiments show that our method outperforms the state-of-the-art method in terms of segmentation and labeling accuracy and is significantly faster, enabling further integration in interactive drawing systems. We demonstrate the efficiency of our method in a sketch-based modeling application that automatically transforms input sketches into 3-D models by part assembly.</p>
          </div>
        </div>
      </div>
</li>
</ol>
            <div class="row">
<div class="col-sm-3"></div>
<div class="col-sm-9"><a class="title" href="/research/">more...</a></div>
</div>
          </div>

        </article>

</div>

    </div>

        <footer class="sticky-bottom mt-5">
      <div class="container">
        © Copyright 2024 Lei  Li. Last updated: March 2024.
      </div>
    </footer>


    
    
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script>
  <script src="/assets/js/common.js"></script>

    
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-P3RT2Z6SLL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-P3RT2Z6SLL');
  </script>
  </body>
</html>

